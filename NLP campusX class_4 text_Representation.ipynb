{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f50ff1",
   "metadata": {},
   "source": [
    "campus x   NLP class 4   Text Representation/vectorization/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c931d96",
   "metadata": {},
   "source": [
    "tf-idf   ,   N-grams    ,  bi-grams and Uni-gramsm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd8850",
   "metadata": {},
   "source": [
    "### most question related to videos \n",
    "1. what is feature Extraction from test ?\n",
    "2. why do we need it?\n",
    "3. why is it difficult ?\n",
    "4. what are the core idea ?\n",
    "5. what are the techniques ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a22bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d121a69",
   "metadata": {},
   "source": [
    "### mathod to convert text into numbers :\n",
    "    1.OneHotEncoding\n",
    "    2. BagOfWords(bog)\n",
    "    3. N-grams\n",
    "    4. Tf-Idf\n",
    "    5.custom Features \n",
    "      (these mathod we will read in this video )\n",
    "    6.word2vec  {embeding}   (it will cover in next video)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbded6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d3a8a43",
   "metadata": {},
   "source": [
    "##### common Terms : that we  use here \n",
    "    corpus (C) = total words in data (repeated also)\n",
    "    vocabulary (V) = unique words only \n",
    "    Document (D) =  single part of data (like review data every review is document)\n",
    "    word (W) = every individual word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87464a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proper notes is  in sql note book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9bef286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.O.W "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba6b41cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd73c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text':['people watch campusx','campusx watch campusx','people write comment','campusx write comment'],'output':[1,1,0,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06b96384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch campusx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>campusx watch campusx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>campusx write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text  output\n",
       "0   people watch campusx       1\n",
       "1  campusx watch campusx       1\n",
       "2   people write comment       0\n",
       "3  campusx write comment       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22e38863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1352edc",
   "metadata": {},
   "outputs": [],
   "source": [
    " bow = cv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b19627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 2, 'watch': 3, 'campusx': 0, 'write': 4, 'comment': 1}\n"
     ]
    }
   ],
   "source": [
    "# vocab\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96ee2b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 0]]\n",
      "[[2 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray())\n",
    "print(bow[1].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2289422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go and read online about Countectorizer on sklearn document\n",
    "# read about max_fearture and  binary = True/False   in this document specially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7243290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc96361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c271f881",
   "metadata": {},
   "source": [
    "## N-grams :\n",
    "    bi-grams\n",
    "    tri-grams\n",
    "    etc....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1df74a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we take n = 2  means bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "677ccfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf086301",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CountVectorizer(ngram_range=(2,2))      # by given ngram_range paramtr =(2,2) it workk as bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c7a81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_grm = cv2.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e41e27ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people watch': 2,\n",
       " 'watch campusx': 4,\n",
       " 'campusx watch': 0,\n",
       " 'people write': 3,\n",
       " 'write comment': 5,\n",
       " 'campusx write': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f94ca93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e484c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what if i give  ngram_range = (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7156e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv3 = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02b005b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_grm2 = cv3.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d01f707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': 4,\n",
       " 'watch': 7,\n",
       " 'campusx': 0,\n",
       " 'people watch': 5,\n",
       " 'watch campusx': 8,\n",
       " 'campusx watch': 1,\n",
       " 'write': 9,\n",
       " 'comment': 3,\n",
       " 'people write': 6,\n",
       " 'write comment': 10,\n",
       " 'campusx write': 2}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv3.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4ff07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it give  single word and two pair of word also "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897c740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea79a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "684fbb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv4 = CountVectorizer(ngram_range=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad0658bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_grm = cv4.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33e5d939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people watch campusx': 2,\n",
       " 'campusx watch campusx': 0,\n",
       " 'people write comment': 3,\n",
       " 'campusx write comment': 1}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv4.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d7a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71ec87c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv5 = CountVectorizer(ngram_range=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30b22c50",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m quard_gram \u001b[38;5;241m=\u001b[39m cv5\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m             )\n\u001b[0;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1289\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1287\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1291\u001b[0m         )\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "quard_gram = cv5.fit_transform(df[\"text\"])     # it give error bcz we have only 3 words in our sentense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d852ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f88a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5b37678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f72beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
